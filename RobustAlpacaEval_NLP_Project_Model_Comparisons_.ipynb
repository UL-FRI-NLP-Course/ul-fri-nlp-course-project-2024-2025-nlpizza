{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ffddbf3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffddbf3a",
        "outputId": "76b79afd-21a5-46f6-8006-3a3044b99d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers huggingface_hub accelerate bitsandbytes pandas sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ae5adff6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae5adff6",
        "outputId": "fdb72832-f435-46d8-e97b-b4848f748b9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "drive           113G   45G   68G  40% /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "os.environ['HF_HOME'] = '/content/drive/MyDrive/hf_cache'\n",
        "os.environ['HUGGINGFACE_HUB_CACHE'] = '/content/drive/MyDrive/huggingface_cache'\n",
        "\n",
        "!rm -rf /content/sample_data  # free space\n",
        "!df -h | grep drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9ea315d1",
      "metadata": {
        "id": "9ea315d1"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_threshold=6.0,\n",
        "    llm_int8_enable_fp32_cpu_offload=True,\n",
        ")\n",
        "\n",
        "def load_model_safely(model_name, quantize=False):\n",
        "    try:\n",
        "        print(f\"ğŸ” Checking for local copy of: {model_name}\")\n",
        "        tok = AutoTokenizer.from_pretrained(model_name, cache_dir=os.environ['HUGGINGFACE_HUB_CACHE'])\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            cache_dir=os.environ['HUGGINGFACE_HUB_CACHE'],\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=(bnb_config if quantize else None),\n",
        "        )\n",
        "        pipe = pipeline(\"text-generation\", model=model, tokenizer=tok, max_new_tokens=128, device_map=\"auto\", do_sample=False)\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Failed to load {model_name}: {e}\")\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c3e87a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178,
          "referenced_widgets": [
            "14a7f8e13ca1408c9def3494aac53565",
            "cfa19ae96d0b451990a54ff18c2f3c39",
            "90b19248465443b3b75e5985425aaa23",
            "c47b116f12194d55bdf60a77a2d21517",
            "afc900c48eef4317930781d51e2e0bd6",
            "809fbf6e20cc4759b909b0e1d6099aa5",
            "7a7063a3c7c649fdaa0cb42bae38a218",
            "cc8d13f2132f44e486cd11148045e599",
            "18dbf5e6b37f48b8bd9cecd8eedda155",
            "ef455b0fff1f4520818d878fe78652fa",
            "c74820c1231f43cd933f26522f3ef716"
          ]
        },
        "id": "2c3e87a6",
        "outputId": "ad7f82e7-5765-4278-c5b8-06e050884993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14a7f8e13ca1408c9def3494aac53565"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "JUDGE_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "judge_tok = AutoTokenizer.from_pretrained(JUDGE_NAME, cache_dir=os.environ['HUGGINGFACE_HUB_CACHE'])\n",
        "judge_model = AutoModelForCausalLM.from_pretrained(JUDGE_NAME, cache_dir=os.environ['HUGGINGFACE_HUB_CACHE']).cpu()\n",
        "\n",
        "judge_gen = pipeline(\"text-generation\", model=judge_model, tokenizer=judge_tok, device=-1, max_new_tokens=256, do_sample=False)\n",
        "print(\"âœ… Judge loaded (on CPU)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa97a090",
      "metadata": {
        "id": "fa97a090"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def safe_load_json(filepath, max_bytes=None):\n",
        "    with open(filepath, \"r\") as f:\n",
        "        raw = f.read(max_bytes) if max_bytes else f.read()\n",
        "    safe_json = raw.rstrip(\",\n",
        "\n",
        "\t\") + \"]\"\n",
        "    return json.loads(safe_json)\n",
        "\n",
        "alpaca = safe_load_json(\"alpaca_prompts.json\")\n",
        "# mmlu = safe_load_json(\"mmlu_prompts.json\", max_bytes=140_000_000)  # slice before corrupted area\n",
        "\n",
        "alpaca = alpaca[:1]\n",
        "alpaca[0][\"prompts\"] = alpaca[0][\"prompts\"][:10]\n",
        "# mmlu = mmlu[:1]\n",
        "\n",
        "datasets = {\"alpaca\": alpaca}\n",
        "print({k: len(v) for k, v in datasets.items()})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27d78d54",
      "metadata": {
        "id": "27d78d54"
      },
      "outputs": [],
      "source": [
        "compare_template = \"\"\"Below are two answers to the same question.\n",
        "Answer A:\n",
        "{A}\n",
        "Answer B:\n",
        "{B}\n",
        "Which one is better, A or B, at addressing the question? Respond with exactly one token: A, B, or Tie.\"\"\".strip()\n",
        "\n",
        "generation_cache = {}\n",
        "\n",
        "def get_cached_response(generator, prompt, model_name):\n",
        "    key = (model_name, prompt)\n",
        "    if key not in generation_cache:\n",
        "        generation_cache[key] = generator(prompt)[0][\"generated_text\"][len(prompt):].strip()\n",
        "    return generation_cache[key]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f1aa17c",
      "metadata": {
        "id": "3f1aa17c"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "results = []\n",
        "\n",
        "TARGET_MODELS = {\n",
        "    \"gpt2\": \"gpt2\",\n",
        "    \"distilgpt2\": \"distilgpt2\",\n",
        "    \"tiny-gpt2\": \"sshleifer/tiny-gpt2\"\n",
        "}\n",
        "\n",
        "for ds_name, data in datasets.items():\n",
        "    for ex in data:\n",
        "        for model_name, model_id in TARGET_MODELS.items():\n",
        "            print(f\"ğŸš€ Evaluating {model_name}\")\n",
        "            try:\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "                time.sleep(2)\n",
        "\n",
        "                gen = load_model_safely(model_id, quantize=False)\n",
        "                if gen is None:\n",
        "                    continue\n",
        "\n",
        "                scores = []\n",
        "                for prompt in ex[\"prompts\"]:\n",
        "                    model_output = get_cached_response(gen, prompt, model_name)\n",
        "                    gold_output = get_cached_response(judge_gen, prompt, \"judge\")\n",
        "                    cmp_prompt = compare_template.format(A=model_output, B=gold_output)\n",
        "                    vote = judge_gen(cmp_prompt)[0][\"generated_text\"].strip().split()[0]\n",
        "                    scores.append(1 if vote in (\"A\", \"Tie\") else 0)\n",
        "\n",
        "                del gen\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                arr = np.array(scores)\n",
        "                results.append({\n",
        "                    \"dataset\": ds_name,\n",
        "                    \"model\": model_name,\n",
        "                    \"id\": ex[\"id\"],\n",
        "                    \"original_score\": float(arr[0]),\n",
        "                    \"worst_score\": float(arr.min()),\n",
        "                    \"best_score\": float(arr.max()),\n",
        "                    \"average_score\": float(arr.mean()),\n",
        "                    \"std_score\": float(arr.std())\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"ğŸ’¥ Skipped {model_name} due to error: {e}\")\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "                time.sleep(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2128f635",
      "metadata": {
        "id": "2128f635"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "print(df)\n",
        "\n",
        "pivot = df.pivot(index=\"model\", columns=\"dataset\", values=\"average_score\")\n",
        "ax = pivot.plot.barh(figsize=(8, 4))\n",
        "ax.set_title(\"Average Score (model â‰¥ gold)\")\n",
        "ax.set_xlabel(\"Score\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14a7f8e13ca1408c9def3494aac53565": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cfa19ae96d0b451990a54ff18c2f3c39",
              "IPY_MODEL_90b19248465443b3b75e5985425aaa23",
              "IPY_MODEL_c47b116f12194d55bdf60a77a2d21517"
            ],
            "layout": "IPY_MODEL_afc900c48eef4317930781d51e2e0bd6"
          }
        },
        "cfa19ae96d0b451990a54ff18c2f3c39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_809fbf6e20cc4759b909b0e1d6099aa5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7a7063a3c7c649fdaa0cb42bae38a218",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡â€‡â€‡0%"
          }
        },
        "90b19248465443b3b75e5985425aaa23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc8d13f2132f44e486cd11148045e599",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18dbf5e6b37f48b8bd9cecd8eedda155",
            "value": 0
          }
        },
        "c47b116f12194d55bdf60a77a2d21517": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef455b0fff1f4520818d878fe78652fa",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c74820c1231f43cd933f26522f3ef716",
            "value": "â€‡0/2â€‡[00:00&lt;?,â€‡?it/s]"
          }
        },
        "afc900c48eef4317930781d51e2e0bd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "809fbf6e20cc4759b909b0e1d6099aa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a7063a3c7c649fdaa0cb42bae38a218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc8d13f2132f44e486cd11148045e599": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18dbf5e6b37f48b8bd9cecd8eedda155": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef455b0fff1f4520818d878fe78652fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c74820c1231f43cd933f26522f3ef716": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}