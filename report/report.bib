@article{Demsar2016BalancedMixture,
    title = {{A Balanced Mixture of Antagonistic Pressures Promotes the Evolution of Parallel Movement}},
    year = {2016},
    journal = {Scientific Reports},
    author = {Dem{\v{s}}ar, Jure and {\v{S}}trumbelj, Erik and Lebar Bajec, Iztok},
    volume = {6},
    doi = {10.1038/srep39428}
}

@article{Demsar2017LinguisticEvolution,
    title = {{Evolution of Collective Behaviour in an Artificial World Using Linguistic Fuzzy Rule-Based Systems}},
    year = {2017},
    journal = {PLoS ONE},
    author = {Dem{\v{s}}ar, Jure and Lebar Bajec, Iztok},
    number = {1},
    pages = {1--20},
    volume = {12},
    doi = {10.1371/journal.pone.0168876}
}

% First paper on the docs
@article{mizrahi-etal-2024-state,
    title = "State of What Art? A Call for Multi-Prompt {LLM} Evaluation",
    author = "Mizrahi, Moran  and
      Kaplan, Guy  and
      Malkin, Dan  and
      Dror, Rotem  and
      Shahaf, Dafna  and
      Stanovsky, Gabriel",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "12",
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.tacl-1.52/",
    doi = "10.1162/tacl_a_00681",
    pages = "933--949",
    abstract = "Recent advances in LLMs have led to an abundance of evaluation benchmarks, which typically rely on a single instruction template per task. We create a large-scale collection of instruction paraphrases and comprehensively analyze the brittleness introduced by single-prompt evaluations across 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. We find that different instruction templates lead to very different performance, both absolute and relative. Instead, we propose a set of diverse metrics on multiple instruction paraphrases, specifically tailored for different use cases (e.g., LLM vs. downstream development), ensuring a more reliable and meaningful assessment of LLM capabilities. We show that our metrics provide new insights into the strengths and limitations of current LLMs."
}

% Second paper on the docs

@misc{cao2024worstpromptperformancelarge,
      title={On the Worst Prompt Performance of Large Language Models}, 
      author={Bowen Cao and Deng Cai and Zhisong Zhang and Yuexian Zou and Wai Lam},
      year={2024},
      eprint={2406.10248},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.10248}, 
}

% Third paper on the docs

@inproceedings{chatterjee-etal-2024-posix,
    title = "{POSIX}: A Prompt Sensitivity Index For Large Language Models",
    author = "Chatterjee, Anwoy  and
      Renduchintala, H S V N S Kowndinya  and
      Bhatia, Sumit  and
      Chakraborty, Tanmoy",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.852/",
    doi = "10.18653/v1/2024.findings-emnlp.852",
    pages = "14550--14565",
    abstract = "Despite their remarkable capabilities, Large Language Models (LLMs) are found to be surprisingly sensitive to minor variations in prompts, often generating significantly divergent outputs in response to minor variations in the prompts, such as spelling errors, alteration of wording or the prompt template. However, while assessing the quality of an LLM, the focus often tends to be solely on its performance on downstream tasks, while very little to no attention is paid to prompt sensitivity. To fill this gap, we propose POSIX {--} a novel PrOmpt Sensitivity IndeX as a reliable measure of prompt sensitivity, thereby offering a more comprehensive evaluation of LLM performance. The key idea behind POSIX is to capture the relative change in loglikelihood of a given response upon replacing the corresponding prompt with a different intent-preserving prompt. We provide thorough empirical evidence demonstrating the efficacy of POSIX in capturing prompt sensitivity and subsequently use it to measure and thereby compare prompt sensitivity of various open source LLMs. We find that merely increasing the parameter count or instruction tuning does not necessarily reduce prompt sensitivity whereas adding some few-shot exemplars, even just one, almost always leads to significant decrease in prompt sensitivity. We also find that alterations to prompt template lead to the highest sensitivity in the case of MCQ type tasks, whereas paraphrasing results in the highest sensitivity in open-ended generation tasks. The code for reproducing our results is open-sourced at https://github.com/kowndinya-renduchintala/POSIX."
}

\% prosa
@article{zhuo2024prosa,
  title={ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs},
  author={Zhuo, Jingming and Zhang, Songyang and Fang, Xinyu and Duan, Haodong and Lin, Dahua and Chen, Kai},
  journal={arXiv preprint arXiv:2410.12405},
  year={2024}
}

\% cot and general knowledge

@misc{lu2024promptsdifferenttermssensitivity,
      title={How are Prompts Different in Terms of Sensitivity?}, 
      author={Sheng Lu and Hendrik Schuff and Iryna Gurevych},
      year={2024},
      eprint={2311.07230},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.07230}, 
}


\% controlled tex generation

@misc{zhou2023controlledtextgenerationnatural,
      title={Controlled Text Generation with Natural Language Instructions}, 
      author={Wangchunshu Zhou and Yuchen Eleanor Jiang and Ethan Wilcox and Ryan Cotterell and Mrinmaya Sachan},
      year={2023},
      eprint={2304.14293},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.14293}, 
}

@misc{yang2024assessingadversarialrobustnesslarge,
      title={Assessing Adversarial Robustness of Large Language Models: An Empirical Study}, 
      author={Zeyu Yang and Zhao Meng and Xiaochen Zheng and Roger Wattenhofer},
      year={2024},
      eprint={2405.02764},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.02764}, 
}