%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
% 
% Jure Demšar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://crInitial implementation / baseline with results - Updated Submission 1 parts - Implemented at least one solution with analysis - Future directions and ideas - Well organized repository eativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}

\graphicspath{{fig/}}




%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------
\usepackage{svg}

% Header
\JournalInfo{FRI Natural language processing course 2025}

% Interim or final report
\Archive{Project report} 
%\Archive{Final report} 

% Article title
\PaperTitle{Improving Prompt Sensitivity of LLMs} 

% Authors (student competitors) and their info
\Authors{Gonçalo Cardoso, Gopika Krishnan, Ali Muhammad}

% Advisors
\affiliation{\textit{Advisors: }}

% Keywords
\Keywords{Keyword1, Keyword2, Keyword3 ...}
\newcommand{\keywordname}{Keywords}


%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{
The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here.
}

%----------------------------------------------------------------------------------------

\begin{document}
\sloppy
% Makes all text pages the same height
\flushbottom 

% Print the title and abstract box
\maketitle 

% Removes page numbering from the first page
\thispagestyle{empty} 

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}
Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. However, a significant challenge in their practical application is their sensitivity to subtle variations in user prompts. Even minor changes in phrasing, such as word order or the inclusion of seemingly innocuous words, can lead to drastically different outputs in terms of content, quality, and even correctness. This phenomenon, known as prompt sensitivity, undermines the reliability and consistency of LLMs, hindering their widespread adoption in critical applications. Several studies like Mizrahi et al \cite{mizrahi-etal-2024-state} have highlighted this issue, demonstrating the instability of LLM performance under slight prompt perturbations, the work presented by Cao et al \cite{cao2024worstpromptperformancelarge} shows a difference of up to 45.48\% on performance for two paraphrased prompts. Understanding and mitigating prompt sensitivity is therefore crucial for building robust and dependable LLM-based systems.

Prompt sensitivity has garnered increasing attention in the research community, leading to the development of various evaluation metrics and mitigation strategies.

Chatterjee et al \cite{chatterjee-etal-2024-posix} introduced POSIX, a metric to evaluate the output invariance of LLMs across paraphrased prompts. ROBUSTALPACAEVAL (Cao et al \cite{cao2024worstpromptperformancelarge}), provides a benchmark for evaluating the robustness of instruction-following models against adversarial prompt variations by comparing model performances. Similarly, PromptSensiScore (Zhuo et al \cite{zhuo2024prosa}) offers a measure to assess the degree to which model outputs change with small prompt modifications. Mizrahi et al \cite{mizrahi-etal-2024-state} further introduce metrics such as MaxP, AvgP, and CPS to quantify both the peak and consistency of model performance across prompt variations. These metrics provide valuable tools for analyzing and comparing the prompt sensitivity of different LLMs and prompting techniques. 

Researchers have explored various approaches to reduce the sensitivity of LLMs to prompt variations. Techniques like voting, where multiple responses generated from slightly different prompts are aggregated to produce a more stable and reliable output and self-refinement techniques, where the model iteratively refines its own output based on different prompt variations have been tested by Cao et al \cite{cao2024worstpromptperformancelarge} with non promising results although voting showcases a stable performance. 
Prompting strategies such as Few-Shot learning (Zhuo et al \cite{zhuo2024prosa}), Chain-of-Thought Prompting (Lu et al \cite{lu2024promptsdifferenttermssensitivity}), and General Knowledge Prompting (Lu et al \cite{lu2024promptsdifferenttermssensitivity}) aim to guide the model towards more consistent and accurate responses by providing relevant context or reasoning steps within the prompt itself. The results of these strategies are more promising but there is a lack of generality for different models.  
	


%------------------------------------------------

\section*{Planned Methods}

To investigate and mitigate prompt sensitivity in LLMs, we will benchmark different prompting strategies, evaluate reasoning models for robustness, and explore fine-tuning for controlled text generation. We will conduct experiments on open-source models that can be manipulated locally.

The language models used in this study are:
\begin{itemize}
    \item Meta AI's second-generation Llama model with 7 billion parameters, fine-tuned for chat and hosted on Hugging Face. \textbf{Technical Name:} Llama-2-7b-chat-hf
    \item A compact yet capable language model developed by MosaicML. \textbf{Technical Name:} mosaicml/phi2
    \item A 6.7 billion parameter model from DeepSeek AI, specifically designed and instruction-tuned for code-related tasks. \textbf{Technical Name:} deepseek-coder-6.7b-instruct
    \item Mistral AI's 7 billion parameter model that has been instruction-tuned. "v0.1" indicates the initial release. \textbf{Technical Name:} mistral-7b-instruct-v0.1
\end{itemize}

We will first test prompting strategies like Few-Shot Learning, Chain-of-Thought Prompting, General Knowledge Prompting, voting, and self-refinement. By comparing outputs across paraphrased prompts, we aim to assess their impact on response stability. Additionally, we will evaluate reasoning models, such as DeepSeek 7B and LLaMA 3, which have been shown to exhibit stronger logical consistency, to determine whether their structured reasoning capabilities contribute to lower sensitivity to prompt variations. We have not found literature of this type of work so we think it can be promising.

As another of the tools to bring insights into prompt sensitivity, we plan to use adversarial prompting to systematically vary input phrasing while preserving intent.  Recent work has framed this brittleness in terms of adversarial robustness, showing that large language models are highly sensitive to carefully crafted prompt variations in attack scenarios (Yang et al \cite{yang2024assessingadversarialrobustnesslarge}). By drawing from this field, we aim to surface inconsistencies in model behavior and use these insights to inform our broader methodology for improving prompt stability.


Finally, we will explore fine tuning techniques, such as the ones presented on Zhou et al \cite{zhou2023controlledtextgenerationnatural} to understand the impact of controlled text generation on prompt sensitivity.

%------------------------------------------------

\section*{Datasets}

For our project on improving prompt sensitivity in LLMs, we have collected diverse prompt datasets from three key repositories that offer rich, data-centric resources:

\textbf{Multi-Prompt LLM Evaluation Dataset:} This repository contains an extensive collection of automatically generated and manually validated instruction paraphrases. The dataset is organized into CSV files each corresponding to specific tasks from benchmarks such as LMentry and BBH. These files include detailed metrics such as model accuracies, correctness assessments, and ranking information for each paraphrase. Additionally, JSON files with aggregated results are provided, allowing for statistical analysis of how different prompt phrasings impact model performance. This dataset is introduced in \cite{mizrahi-etal-2024-state}.\\
\textbf{Link:} \url{https://github.com/SLAB-NLP/Multi-Prompt-LLM-Evaluation}

\vspace{1em}
\textbf{POSIX Dataset:} In addition to its code, the POSIX repository supplies datasets comprising sets of intent-aligned prompts paired with model-generated responses. Each dataset includes multiple variations of the same query, covering modifications in wording, spelling, and template structure, which enable us to compute key metrics such as response diversity, distribution entropy, semantic coherence, and confidence variance. This dataset forms the backbone of the quantitative analysis presented in \cite{chatterjee-etal-2024-posix}.\\
\textbf{Link:} \url{https://github.com/kowndinya-renduchintala/POSIX}

\vspace{1em}
\textbf{RobustAlpacaEval Dataset:} This repository provides a benchmark dataset specifically designed to assess worst-case prompt performance. Based on a refined subset of the TinyAlpacaEval benchmark, the dataset features 10 manually refined paraphrases per query. This dataset is presented in \cite{cao2024worstpromptperformancelarge}.\\
\textbf{Link:} \url{https://github.com/bwcao/RobustAlpacaEval}

Collectively, these datasets provide a robust foundation for analyzing the variability in LLM performance across diverse prompt formulations, a critical component for our study on reducing prompt sensitivity.



%------------------------------------------------

\section*{Results (Intermediate Status)}

Our initial implementation focused on evaluating prompt sensitivity using a custom version of the Prompt Sensitivity Index (POSIX) \cite{chatterjee-etal-2024-posix}. The official GitHub repository provided limited utility, so we opted for a clean and modular implementation tailored to our setup. Our version closely adheres to the original POSIX computation logic, measuring the log-probability divergence of model responses across paraphrased prompts. We ran experiments on the RobustAlpacaEval dataset (4996 prompt sets) using the \texttt{tiiuae/falcon-rw-1b} model. The POSIX pipeline was executed via SLURM array jobs on the HPC cluster, with 46 parallel jobs processing 100 sets each.

We successfully generated baseline responses and calculated POSIX scores for each prompt set. Preliminary analysis shows substantial variability across prompt sets, with some exhibiting high sensitivity to rewording. To address this, we implemented a Chain-of-Thought (CoT) variant by modifying each prompt with the suffix ``\textit{Let's think step by step.}''

\subsection*{POSIX Improvements via Chain-of-Thought Prompting}

Quantitative results show a clear improvement in output stability. The average POSIX score dropped from 1.25 in the baseline to 0.59 with CoT prompting (a 47\% reduction in sensitivity). Figure~\ref{fig:posix_delta_hist} shows the full distribution of POSIX scores for both the baseline and CoT variants. The distribution for CoT is sharply left-shifted, indicating more consistent and stable responses across paraphrased prompts. CoT also eliminates several extreme outliers present in the baseline.

\begin{figure}[h]
    \centering
    \includesvg[width=\linewidth]{figs/posix_delta_hist_cot_vs_baseline}
    \caption{Histogram of POSIX score changes (CoT – Baseline). The majority of values are negative, indicating increased output stability under Chain-of-Thought prompting.}
    \label{fig:posix_delta_hist}
\end{figure}

\subsection*{Sensitivity to Dataset Formatting Conventions}

A qualitative examination of our prompt sets revealed that even the original dataset formatting (featuring explicit role indicators like \texttt{Q:}, \texttt{A:}, \texttt{question:}, or \texttt{answer:}) frequently confused the models. Rather than guiding the model toward cleaner completions, these structural cues often led to repetitive or shallow outputs, particularly in the baseline setting. 

Adding Chain-of-Thought (CoT) prompting introduced structural regularity, often shifting responses to a numbered list format. This improved factuality and variety in cases where the prompt was only lightly paraphrased. However, when prompts included deeper noise, such as misspellings (e.g., \texttt{lisit okf tean interesting thingd}) or stylized tokens like \texttt{Q:::} or \texttt{||}, CoT completions remained brittle. In several cases, outputs hallucinated irrelevant entities or defaulted to unrelated QA-style answers (e.g., ``What is the name of the file?'').

These findings suggest that while CoT prompting increases stability under superficial rewording, it does not provide robustness against noisier or syntactically corrupted inputs. The model’s behavior is sensitive not only to meaning but also to formatting patterns that disrupt its internal structure parsing.


% We have also implemented semantic similarity scoring using Sentence Transformers to assess semantic drift across paraphrased prompts. This analysis is being finalized. In addition to measuring surface-level similarity, we aim to explore convergence depth: whether Chain-of-Thought prompting leads the model to settle on a consistent line of reasoning across paraphrased prompts. Rather than counting steps alone, we will analyze how early in the reasoning process responses begin to align, and how this convergence varies across different types of prompt variations. This will help us understand whether CoT stabilizes not just outputs, but the underlying reasoning trajectory, and under what conditions it fails to do so.



%------------------------------------------------
\section*{Discussion and Next Steps}

Our POSIX-based evaluation confirms that prompt phrasing significantly influences model behavior. Chain-of-Thought (CoT) prompting reduced sensitivity in many cases, but failures persisted under noisy or unconventional prompt structures.

To build on this, we plan to:

\begin{itemize}[itemsep=2pt]
    \item \textbf{Explore iterative self-checking:} Re-prompt the model over $k$ steps, each time including its previous output, to encourage internal consistency. We'll analyze performance-vs-cost trade-offs.
    
    \item \textbf{Test self-refinement:} Let the model rewrite prompts before answering, using instructions like \textit{``Rephrase the question and solve it.’’}

    \item \textbf{Evaluate stronger models:} Run the same experiments on Mistral and LLaMA to assess how model scale affects sensitivity.

    \item \textbf{Incorporate additional evaluation metrics:} Alongside POSIX, we plan to experiment with other evaluation methods presented in earlier works to provide a more comprehensive picture of model robustness.

    \item \textbf{Introduce adversarial variants:} Generate controlled corruptions (e.g., typos, misplaced punctuation, odd structure) to explicitly stress-test robustness across methods.
\end{itemize}

These steps will help us better understand both the limitations of current prompting strategies and which combinations of techniques and models best improve response stability and meaning retention.

%------------------------------------------------

\section*{Acknowledgments}

Here you can thank other persons (advisors, colleagues ...) that contributed to the successful completion of your project.


%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{report}


\end{document}

