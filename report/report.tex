%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
% 
% Jure Demšar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}

\graphicspath{{fig/}}




%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% Header
\JournalInfo{FRI Natural language processing course 2025}

% Interim or final report
\Archive{Project report} 
%\Archive{Final report} 

% Article title
\PaperTitle{Improving prompt sensitivity of LLMs} 

% Authors (student competitors) and their info
\Authors{Gonçalo Cardoso, Gopika Krishnan, Ali Muhammad}

% Advisors
\affiliation{\textit{Advisors: }}

% Keywords
\Keywords{Keyword1, Keyword2, Keyword3 ...}
\newcommand{\keywordname}{Keywords}


%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{
The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here. The abstract goes here.
}

%----------------------------------------------------------------------------------------

\begin{document}

% Makes all text pages the same height
\flushbottom 

% Print the title and abstract box
\maketitle 

% Removes page numbering from the first page
\thispagestyle{empty} 

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}
Large Language Models (LLMs) have demonstrated remarkable capabilities across various natural language processing tasks. However, a significant challenge in their practical application is their sensitivity to subtle variations in user prompts. Even minor changes in phrasing, such as word order or the inclusion of seemingly innocuous words, can lead to drastically different outputs in terms of content, quality, and even correctness. This phenomenon, known as prompt sensitivity, undermines the reliability and consistency of LLMs, hindering their widespread adoption in critical applications. Several studies like Mizrahi et al \cite{mizrahi-etal-2024-state} have highlighted this issue, demonstrating the instability of LLM performance under slight prompt perturbations, the work presented by Cao et al \cite{cao2024worstpromptperformancelarge} shows a difference of up to 45.48\% on performance for two paraphrased prompts. Understanding and mitigating prompt sensitivity is therefore crucial for building robust and dependable LLM-based systems.

Prompt sensitivity has garnered increasing attention in the research community, leading to the development of various evaluation metrics and mitigation strategies.

Chatterjee et al \cite{chatterjee-etal-2024-posix} introduced POSIX, a metric to evaluate the output invariance of LLMs across paraphrased prompts. ROBUSTALPACAEVAL (Cao et al \cite{cao2024worstpromptperformancelarge}), provides a benchmark for evaluating the robustness of instruction-following models against adversarial prompt variations by comparing model performances. Similarly, PromptSensiScore (Zhuo et al \cite{zhuo2024prosa}) offers a measure to assess the degree to which model outputs change with small prompt modifications. Mizrahi et al \cite{mizrahi-etal-2024-state} further introduce metrics such as MaxP, AvgP, and CPS to quantify both the peak and consistency of model performance across prompt variations. These metrics provide valuable tools for analyzing and comparing the prompt sensitivity of different LLMs and prompting techniques. 

Researchers have explored various approaches to reduce the sensitivity of LLMs to prompt variations. Techniques like voting, where multiple responses generated from slightly different prompts are aggregated to produce a more stable and reliable output and self-refinement techniques, where the model iteratively refines its own output based on different prompt variations have been tested by Cao et al \cite{cao2024worstpromptperformancelarge} with non promising results although voting showcases a stable performance. 
Prompting strategies such as Few-Shot learning (Zhuo et al \cite{zhuo2024prosa}), Chain-of-Thought Prompting (Lu et al \cite{lu2024promptsdifferenttermssensitivity}), and General Knowledge Prompting (Lu et al \cite{lu2024promptsdifferenttermssensitivity}) aim to guide the model towards more consistent and accurate responses by providing relevant context or reasoning steps within the prompt itself. The results of these strategies are more promising but there is a lack of generality for different models.  
	


%------------------------------------------------

\section*{Planned Methods}

To investigate and mitigate prompt sensitivity in LLMs, we will benchmark different prompting strategies, evaluate reasoning models for robustness, and explore fine-tuning for controlled text generation. We will conduct experiments on open-source models that can be manipulated locally, such as LLaMA, Mistral and Deepseek models.

We will first test prompting strategies like Few-Shot Learning, Chain-of-Thought Prompting, General Knowledge Prompting, voting, and self-refinement. By comparing outputs across paraphrased prompts, we aim to assess their impact on response stability. Additionally, we will evaluate reasoning models, such as DeepSeek 7B and LLaMA 3, which have been shown to exhibit stronger logical consistency, to determine whether their structured reasoning capabilities contribute to lower sensitivity to prompt variations. We have not found literature of this type of work so we think it can be promising.

As another of the tools to bring insights into prompt sensitivity, we plan to use adversarial prompting to systematically vary input phrasing while preserving intent.  Recent work has framed this brittleness in terms of adversarial robustness, showing that large language models are highly sensitive to carefully crafted prompt variations in attack scenarios (Yang et al \cite{yang2024assessingadversarialrobustnesslarge}). By drawing from this field, we aim to surface inconsistencies in model behavior and use these insights to inform our broader methodology for improving prompt stability.


Finally, we will explore fine tuning techniques, such as the ones presented on Zhou et al \cite{zhou2023controlledtextgenerationnatural} to understand the impact of controlled text generation on prompt sensitivity.

%------------------------------------------------

\section*{Datasets}

For our project on improving prompt sensitivity in LLMs, we have collected diverse prompt datasets from three key repositories that offer rich, data-centric resources:

\textbf{Multi-Prompt LLM Evaluation Dataset:} This repository contains an extensive collection of automatically generated and manually validated instruction paraphrases. The dataset is organized into CSV files each corresponding to specific tasks from benchmarks such as LMentry and BBH. These files include detailed metrics such as model accuracies, correctness assessments, and ranking information for each paraphrase. Additionally, JSON files with aggregated results are provided, allowing for statistical analysis of how different prompt phrasings impact model performance. This dataset is introduced in \cite{mizrahi-etal-2024-state}.\\
\textbf{Link:} \url{https://github.com/SLAB-NLP/Multi-Prompt-LLM-Evaluation}

\vspace{1em}
\textbf{POSIX Dataset:} In addition to its code, the POSIX repository supplies datasets comprising sets of intent-aligned prompts paired with model-generated responses. Each dataset includes multiple variations of the same query, covering modifications in wording, spelling, and template structure, which enable us to compute key metrics such as response diversity, distribution entropy, semantic coherence, and confidence variance. This dataset forms the backbone of the quantitative analysis presented in \cite{chatterjee-etal-2024-posix}.\\
\textbf{Link:} \url{https://github.com/kowndinya-renduchintala/POSIX}

\vspace{1em}
\textbf{RobustAlpacaEval Dataset:} This repository provides a benchmark dataset specifically designed to assess worst-case prompt performance. Based on a refined subset of the TinyAlpacaEval benchmark, the dataset features 10 manually refined paraphrases per query. This dataset is presented in \cite{cao2024worstpromptperformancelarge}.\\
\textbf{Link:} \url{https://github.com/bwcao/RobustAlpacaEval}

Collectively, these datasets provide a robust foundation for analyzing the variability in LLM performance across diverse prompt formulations, a critical component for our study on reducing prompt sensitivity.



%------------------------------------------------

\section*{Results}

Use the results section to present the final results of your work. Present the results in a objective and scientific fashion. Use visualisations to convey your results in a clear and efficient manner. When comparing results between various techniques use appropriate statistical methodology.



%------------------------------------------------

\section*{Discussion}

Use the Discussion section to objectively evaluate your work, do not just put praise on everything you did, be critical and exposes flaws and weaknesses of your solution. You can also explain what you would do differently if you would be able to start again and what upgrades could be done on the project in the future.


%------------------------------------------------

\section*{Acknowledgments}

Here you can thank other persons (advisors, colleagues ...) that contributed to the successful completion of your project.


%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{report}


\end{document}

